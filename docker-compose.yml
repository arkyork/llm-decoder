
services:
  llm_service:
    build: .
    command: ["tail", "-f", "/dev/null"]
    volumes:
      - .:/app
      - /app/.venv
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]